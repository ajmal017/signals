{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading historical forex data from forexite\n",
    "The url I got from [here](https://www.quantshare.com/sa-421-6-places-to-download-historical-intraday-forex-quotes-data-for-free). It has 1m resolution data for [lots](./forexite_pairs.txt) of symbols. Eah file contains one days data for all symbols. I need to rework the data, one file per symbol. Store in pandas and serialize. Need to handle time zones somehow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import zipfile\n",
    "import urllib.request\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.forexite.com/free_forex_quotes/2020/01/170120.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/01/160120.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/281019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/271019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/261019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/251019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/241019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/231019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/221019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/211019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/201019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/191019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/181019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/171019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/161019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/151019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/141019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/131019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/121019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/111019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/101019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/091019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/081019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/071019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/061019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/051019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/041019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/031019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/021019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/10/011019.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/300919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/290919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/280919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/270919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/260919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/250919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/240919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/230919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/220919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/210919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/200919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/190919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/180919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/170919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/160919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/150919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/140919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/130919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/120919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/110919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/100919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/090919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/080919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/070919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/060919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/050919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/040919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/030919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/020919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/09/010919.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/08/310819.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/08/300819.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/08/290819.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/08/280819.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/08/270819.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/08/260819.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/08/250819.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/08/240819.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/08/230819.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/08/220819.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/08/210819.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/08/200819.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/08/190819.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/08/180819.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/08/170819.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/08/160819.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/08/150819.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/08/140819.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/08/130819.zip\n",
      "https://www.forexite.com/free_forex_quotes/2019/08/120819.zip\n"
     ]
    }
   ],
   "source": [
    "date = datetime.datetime.now()\n",
    "# e.g. url = \"https://www.forexite.com/free_forex_quotes/2011/11/011111.zip\"\n",
    "url = \"https://www.forexite.com/free_forex_quotes/{}/{:02d}/{:02d}{:02d}{:02d}.zip\"\n",
    "filename = \"./forexite/{:04d}-{:02d}-{:02d}.zip\"\n",
    "new_files = []\n",
    "\n",
    "for x in range(1, 80):\n",
    "    data_date = date - datetime.timedelta(days=x)\n",
    "    year = data_date.year\n",
    "    year_2 = int(str(year)[-2:])\n",
    "    month = data_date.month\n",
    "    day = data_date.day\n",
    "    download = url.format(year, month, day, month, year_2)\n",
    "    file = filename.format(year, month, day)\n",
    "    file_obj = Path(file)\n",
    "    if not file_obj.exists():\n",
    "        print(download)\n",
    "        if not Path('./forexite').is_dir():\n",
    "            Path('./forexite').mkdir()\n",
    "        urllib.request.urlretrieve(download, file)\n",
    "        new_files.append(file_obj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to csv files, one ticker per file\n",
    "I want one file for each currency pair. I would like it to read the current file, figure out if there is any new data and add it, and then save the file again. I will read and analyse the files in a different codepath.\n",
    "\n",
    "In fact, lets convert the forexite data into csv files first, these are human readable and pandas readable.\n",
    "Thoughts:\n",
    "- Do I dump everything into the csv, then sort and dedupe\n",
    "- Should I use a list of tuples, (datetime, o, h, l, c)\n",
    "- I could load the existing file (if it exists) into a set, each new file can check to see if it is duplicated\n",
    "- I only need to use the date to dedupe, so, from each file, get the date, check the set. If not there, we add all the rows to the list of tuples\n",
    "- Then we sort them and write them out\n",
    "- This might have quite high memory requirements, can we make some assumptions about the order the files are presented to us? Would be challenging if there were gaps in the data, is this an edge case?\n",
    "- Lets start with the first run case, we can optimise later if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes a filename\n",
    "# function that takes a row and a list\n",
    "def parse_row(row_from_file):\n",
    "    # Row looks like:\n",
    "    # EURUSD,20191101,000900,1.1150,1.1150,1.1150,1.1150\n",
    "    tokens = row_from_file.split(',')\n",
    "    assert len(tokens) == 7, f\"Not enough tokens in: {row_from_file}\"\n",
    "    \n",
    "    date_time_str = str(tokens[1]) + str(tokens[2])\n",
    "    dt = datetime.datetime.strptime(date_time_str, '%Y%m%d%H%M%S')\n",
    "    \n",
    "    ticker = tokens[0].strip()\n",
    "    opener = tokens[3].strip()\n",
    "    high = tokens[4].strip()\n",
    "    low = tokens[5].strip()\n",
    "    close = tokens[6].strip()\n",
    "    \n",
    "    return ticker, (dt, opener, high, low, close)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating file list\n",
      "Parsing available tickers and Opening output files\n",
      "Processing 170120.txt\n",
      "Parsing files\n",
      "Processing 201019.txt\n",
      "Processing 211019.txt\n",
      "Processing 221019.txt\n",
      "Processing 231019.txt\n",
      "Processing 241019.txt\n",
      "Processing 251019.txt\n",
      "Processing 261019.txt\n",
      "Processing 271019.txt\n",
      "Processing 281019.txt\n",
      "Processing 291019.txt\n",
      "Processing 301019.txt\n",
      "Processing 311019.txt\n",
      "Processing 011119.txt\n",
      "Processing 021119.txt\n",
      "Processing 031119.txt\n",
      "Processing 041119.txt\n",
      "Processing 051119.txt\n",
      "Processing 061119.txt\n",
      "Processing 071119.txt\n",
      "Processing 081119.txt\n",
      "Processing 091119.txt\n",
      "Processing 101119.txt\n",
      "Processing 111119.txt\n",
      "Processing 121119.txt\n",
      "Processing 131119.txt\n",
      "Processing 141119.txt\n",
      "Processing 151119.txt\n",
      "Processing 161119.txt\n",
      "Processing 171119.txt\n",
      "Processing 181119.txt\n",
      "Processing 191119.txt\n",
      "Processing 201119.txt\n",
      "Processing 211119.txt\n",
      "Processing 221119.txt\n",
      "Processing 231119.txt\n",
      "Processing 241119.txt\n",
      "Processing 251119.txt\n",
      "Processing 261119.txt\n",
      "Processing 271119.txt\n",
      "Processing 281119.txt\n",
      "Processing 291119.txt\n",
      "Processing 301119.txt\n",
      "Processing 011219.txt\n",
      "Processing 021219.txt\n",
      "Processing 031219.txt\n",
      "Processing 041219.txt\n",
      "Processing 051219.txt\n",
      "Processing 061219.txt\n",
      "Processing 071219.txt\n",
      "Processing 081219.txt\n",
      "Processing 091219.txt\n",
      "Processing 101219.txt\n",
      "Processing 111219.txt\n",
      "Processing 121219.txt\n",
      "Processing 131219.txt\n",
      "Processing 141219.txt\n",
      "Processing 151219.txt\n",
      "Processing 161219.txt\n",
      "Processing 171219.txt\n",
      "Processing 181219.txt\n",
      "Processing 191219.txt\n",
      "Processing 201219.txt\n",
      "Processing 211219.txt\n",
      "Processing 221219.txt\n",
      "Processing 231219.txt\n",
      "Processing 241219.txt\n",
      "Processing 251219.txt\n",
      "Processing 261219.txt\n",
      "Processing 271219.txt\n",
      "Processing 281219.txt\n",
      "Processing 291219.txt\n",
      "Processing 301219.txt\n",
      "Processing 311219.txt\n",
      "Processing 010120.txt\n",
      "Processing 020120.txt\n",
      "Processing 030120.txt\n",
      "Processing 040120.txt\n",
      "Processing 050120.txt\n",
      "Processing 060120.txt\n",
      "Processing 070120.txt\n",
      "Processing 080120.txt\n",
      "Processing 090120.txt\n",
      "Processing 100120.txt\n",
      "Processing 110120.txt\n",
      "Processing 120120.txt\n",
      "Processing 130120.txt\n",
      "Processing 140120.txt\n",
      "Processing 150120.txt\n",
      "Processing 160120.txt\n",
      "Processing 170120.txt\n",
      "Closing output files\n"
     ]
    }
   ],
   "source": [
    "file_handles = {}\n",
    "\n",
    "print(\"Generating file list\")\n",
    "new_files = None # HACK!\n",
    "files = new_files\n",
    "if not new_files:\n",
    "    files = Path('./forexite/').glob('*.zip')\n",
    "files = sorted(list(files))\n",
    "\n",
    "\n",
    "if not Path('./csv').is_dir():\n",
    "    print(\"Creating output directory\")\n",
    "    Path('./csv').mkdir()\n",
    "    \n",
    "print(\"Parsing available tickers and Opening output files\")\n",
    "with zipfile.ZipFile(files[-1]) as z:\n",
    "    fn = z.namelist()[0]\n",
    "    print(f\"Processing {fn}\")\n",
    "    with z.open(fn) as f:\n",
    "        next(f)\n",
    "        for row in f:\n",
    "            clean_row = row.decode(\"utf-8\").strip()\n",
    "            ticker, ohlc = parse_row(clean_row)\n",
    "            if ticker not in file_handles:\n",
    "                file_handles[ticker] = open(f\"./csv/{ticker}.csv\", \"a\")\n",
    "\n",
    "print(\"Parsing files\")\n",
    "for file in files:\n",
    "    with zipfile.ZipFile(file) as z:\n",
    "        fn = z.namelist()[0]\n",
    "        print(f\"Processing {fn}\")\n",
    "        with z.open(fn) as f:\n",
    "            next(f)\n",
    "            for row in f:\n",
    "                clean_row = row.decode(\"utf-8\").strip()\n",
    "                ticker, ohlc = parse_row(clean_row)\n",
    "                if ticker not in file_handles:\n",
    "                    continue\n",
    "                file_handles[ticker].write(\"{}, {}, {}, {}, {}\\n\".format(*ohlc))\n",
    "                \n",
    "print(\"Closing output files\")\n",
    "for _, handle in file_handles.items():\n",
    "    handle.close()\n",
    "                \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
