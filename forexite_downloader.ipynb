{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading historical forex data from forexite\n",
    "The url I got from [here](https://www.quantshare.com/sa-421-6-places-to-download-historical-intraday-forex-quotes-data-for-free). It has 1m resolution data for [lots](./forexite_pairs.txt) of symbols. Eah file contains one days data for all symbols. I need to rework the data, one file per symbol. Store in pandas and serialize. Need to handle time zones somehow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import zipfile\n",
    "import urllib.request\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.forexite.com/free_forex_quotes/2020/05/100520.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/05/090520.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/05/080520.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/05/070520.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/05/060520.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/05/050520.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/05/040520.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/05/030520.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/05/020520.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/05/010520.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/300420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/290420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/280420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/270420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/260420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/250420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/240420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/230420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/220420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/210420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/200420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/190420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/180420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/170420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/160420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/150420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/140420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/130420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/120420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/110420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/100420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/090420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/080420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/070420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/060420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/050420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/040420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/030420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/020420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/04/010420.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/03/310320.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/03/300320.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/03/290320.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/03/280320.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/03/270320.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/03/260320.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/03/250320.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/03/240320.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/03/230320.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/03/220320.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/03/210320.zip\n"
     ]
    }
   ],
   "source": [
    "date = datetime.datetime.now()\n",
    "# e.g. url = \"https://www.forexite.com/free_forex_quotes/2011/11/011111.zip\"\n",
    "url = \"https://www.forexite.com/free_forex_quotes/{}/{:02d}/{:02d}{:02d}{:02d}.zip\"\n",
    "filename = \"./forexite/{:04d}-{:02d}-{:02d}.zip\"\n",
    "new_files = []\n",
    "\n",
    "for x in range(1, 365):\n",
    "    data_date = date - datetime.timedelta(days=x)\n",
    "    year = data_date.year\n",
    "    year_2 = int(str(year)[-2:])\n",
    "    month = data_date.month\n",
    "    day = data_date.day\n",
    "    download = url.format(year, month, day, month, year_2)\n",
    "    file = filename.format(year, month, day)\n",
    "    file_obj = Path(file)\n",
    "    if not file_obj.exists():\n",
    "        print(download)\n",
    "        if not Path('./forexite').is_dir():\n",
    "            Path('./forexite').mkdir()\n",
    "        urllib.request.urlretrieve(download, file)\n",
    "        new_files.append(file_obj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to csv files, one ticker per file\n",
    "I want one file for each currency pair. I would like it to read the current file, figure out if there is any new data and add it, and then save the file again. I will read and analyse the files in a different codepath.\n",
    "\n",
    "In fact, lets convert the forexite data into csv files first, these are human readable and pandas readable.\n",
    "Thoughts:\n",
    "- Do I dump everything into the csv, then sort and dedupe\n",
    "- Should I use a list of tuples, (datetime, o, h, l, c)\n",
    "- I could load the existing file (if it exists) into a set, each new file can check to see if it is duplicated\n",
    "- I only need to use the date to dedupe, so, from each file, get the date, check the set. If not there, we add all the rows to the list of tuples\n",
    "- Then we sort them and write them out\n",
    "- This might have quite high memory requirements, can we make some assumptions about the order the files are presented to us? Would be challenging if there were gaps in the data, is this an edge case?\n",
    "- Lets start with the first run case, we can optimise later if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes a filename\n",
    "# function that takes a row and a list\n",
    "def parse_row(row_from_file):\n",
    "    # Row looks like:\n",
    "    # EURUSD,20191101,000900,1.1150,1.1150,1.1150,1.1150\n",
    "    tokens = row_from_file.split(',')\n",
    "    assert len(tokens) == 7, f\"Not enough tokens in: {row_from_file}\"\n",
    "    \n",
    "    date_time_str = str(tokens[1]) + str(tokens[2])\n",
    "    dt = datetime.datetime.strptime(date_time_str, '%Y%m%d%H%M%S')\n",
    "    \n",
    "    ticker = tokens[0].strip()\n",
    "    opener = tokens[3].strip()\n",
    "    high = tokens[4].strip()\n",
    "    low = tokens[5].strip()\n",
    "    close = tokens[6].strip()\n",
    "    \n",
    "    return ticker, (dt, opener, high, low, close)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating file list\n",
      "Parsing available tickers and Opening output files\n",
      "Processing 100520.txt\n",
      "Parsing files\n",
      "Processing 210320.txt\n",
      "Processing 220320.txt\n",
      "Processing 230320.txt\n",
      "Processing 240320.txt\n",
      "Processing 250320.txt\n",
      "Processing 260320.txt\n",
      "Processing 270320.txt\n",
      "Processing 280320.txt\n",
      "Processing 290320.txt\n",
      "Processing 300320.txt\n",
      "Processing 310320.txt\n",
      "Processing 010420.txt\n",
      "Processing 020420.txt\n",
      "Processing 030420.txt\n",
      "Processing 040420.txt\n",
      "Processing 050420.txt\n",
      "Processing 060420.txt\n",
      "Processing 070420.txt\n",
      "Processing 080420.txt\n",
      "Processing 090420.txt\n",
      "Processing 100420.txt\n",
      "Processing 110420.txt\n",
      "Processing 120420.txt\n",
      "Processing 130420.txt\n",
      "Processing 140420.txt\n",
      "Processing 150420.txt\n",
      "Processing 160420.txt\n",
      "Processing 170420.txt\n",
      "Processing 180420.txt\n",
      "Processing 190420.txt\n",
      "Processing 200420.txt\n",
      "Processing 210420.txt\n",
      "Processing 220420.txt\n",
      "Processing 230420.txt\n",
      "Processing 240420.txt\n",
      "Processing 250420.txt\n",
      "Processing 260420.txt\n",
      "Processing 270420.txt\n",
      "Processing 280420.txt\n",
      "Processing 290420.txt\n",
      "Processing 300420.txt\n",
      "Processing 010520.txt\n",
      "Processing 020520.txt\n",
      "Processing 030520.txt\n",
      "Processing 040520.txt\n",
      "Processing 050520.txt\n",
      "Processing 060520.txt\n",
      "Processing 070520.txt\n",
      "Processing 080520.txt\n",
      "Processing 090520.txt\n",
      "Processing 100520.txt\n",
      "Closing output files\n"
     ]
    }
   ],
   "source": [
    "file_handles = {}\n",
    "\n",
    "print(\"Generating file list\")\n",
    "#new_files = None # HACK!\n",
    "files = new_files\n",
    "if not new_files:\n",
    "    files = Path('./forexite/').glob('*.zip')\n",
    "files = sorted(list(files))\n",
    "\n",
    "\n",
    "if not Path('./csv').is_dir():\n",
    "    print(\"Creating output directory\")\n",
    "    Path('./csv').mkdir()\n",
    "    \n",
    "print(\"Parsing available tickers and Opening output files\")\n",
    "with zipfile.ZipFile(files[-1]) as z:\n",
    "    fn = z.namelist()[0]\n",
    "    print(f\"Processing {fn}\")\n",
    "    with z.open(fn) as f:\n",
    "        next(f)\n",
    "        for row in f:\n",
    "            clean_row = row.decode(\"utf-8\").strip()\n",
    "            ticker, ohlc = parse_row(clean_row)\n",
    "            if ticker not in file_handles:\n",
    "                file_handles[ticker] = open(f\"./csv/{ticker}.csv\", \"a\")\n",
    "\n",
    "print(\"Parsing files\")\n",
    "for file in files:\n",
    "    with zipfile.ZipFile(file) as z:\n",
    "        fn = z.namelist()[0]\n",
    "        print(f\"Processing {fn}\")\n",
    "        with z.open(fn) as f:\n",
    "            next(f)\n",
    "            for row in f:\n",
    "                clean_row = row.decode(\"utf-8\").strip()\n",
    "                ticker, ohlc = parse_row(clean_row)\n",
    "                if ticker not in file_handles:\n",
    "                    continue\n",
    "                file_handles[ticker].write(\"{}, {}, {}, {}, {}\\n\".format(*ohlc))\n",
    "                \n",
    "print(\"Closing output files\")\n",
    "for _, handle in file_handles.items():\n",
    "    handle.close()\n",
    "                \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
