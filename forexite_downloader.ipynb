{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading historical forex data from forexite\n",
    "The url I got from [here](https://www.quantshare.com/sa-421-6-places-to-download-historical-intraday-forex-quotes-data-for-free). It has 1m resolution data for [lots](./forexite_pairs.txt) of symbols. Eah file contains one days data for all symbols. I need to rework the data, one file per symbol. Store in pandas and serialize. Need to handle time zones somehow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import zipfile\n",
    "import urllib.request\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.forexite.com/free_forex_quotes/2020/01/120120.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/01/110120.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/01/100120.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/01/090120.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/01/080120.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/01/070120.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/01/060120.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/01/050120.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/01/040120.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/01/030120.zip\n"
     ]
    }
   ],
   "source": [
    "date = datetime.datetime.now()\n",
    "# e.g. url = \"https://www.forexite.com/free_forex_quotes/2011/11/011111.zip\"\n",
    "url = \"https://www.forexite.com/free_forex_quotes/{}/{:02d}/{:02d}{:02d}{:02d}.zip\"\n",
    "filename = \"./forexite/{:04d}-{:02d}-{:02d}.zip\"\n",
    "new_files = []\n",
    "\n",
    "for x in range(1, 365):\n",
    "    data_date = date - datetime.timedelta(days=x)\n",
    "    year = data_date.year\n",
    "    year_2 = int(str(year)[-2:])\n",
    "    month = data_date.month\n",
    "    day = data_date.day\n",
    "    download = url.format(year, month, day, month, year_2)\n",
    "    file = filename.format(year, month, day)\n",
    "    file_obj = Path(file)\n",
    "    if not file_obj.exists():\n",
    "        print(download)\n",
    "        urllib.request.urlretrieve(download, file)\n",
    "        new_files.append(file_obj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to pandas dataframes\n",
    "I want one data frame for each currency pair. I would like it to read the current saved dataframe, figure out if there is any new data and add it, and then save the dataframe again. I will read and analyse the dataframes in a different codepath.\n",
    "\n",
    "In fact, lets convert the forexite data into csv files first, these are human readable and pandas readable.\n",
    "Thoughts:\n",
    "- Do I dump everything into the csv, then sort and dedupe\n",
    "- Should I use a list of tuples, (datetime, o, h, l, c)\n",
    "- I could load the existing file (if it exists) into a set, each new file can check to see if it is duplicated\n",
    "- I only need to use the date to dedupe, so, from each file, get the date, check the set. If not there, we add all the rows to the list of tuples\n",
    "- Then we sort them and write them out\n",
    "- This might have quite high memory requirements, can we make some assumptions about the order the files are presented to us? Would be challenging if there were gaps in the data, is this an edge case?\n",
    "- Lets start with the first run case, we can optimise later if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes a filename\n",
    "# function that takes a row and a list\n",
    "def parse_row(row_from_file):\n",
    "    # Row looks like:\n",
    "    # EURUSD,20191101,000900,1.1150,1.1150,1.1150,1.1150\n",
    "    tokens = row_from_file.split(',')\n",
    "    assert len(tokens) == 7, f\"Not enough tokens in: {row_from_file}\"\n",
    "    \n",
    "    date_time_str = str(tokens[1]) + str(tokens[2])\n",
    "    dt = datetime.datetime.strptime(date_time_str, '%Y%m%d%H%M%S')\n",
    "    \n",
    "    ticker = tokens[0]\n",
    "    opener = tokens[3]\n",
    "    high = tokens[4]\n",
    "    low = tokens[5]\n",
    "    close = tokens[6]\n",
    "    \n",
    "    return ticker, (dt, opener, high, low, close)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening output files\n",
      "Loading file list\n",
      "Parsing files\n",
      "Processing 030120.txt\n",
      "Processing 040120.txt\n",
      "Processing 050120.txt\n",
      "Processing 060120.txt\n",
      "Processing 070120.txt\n",
      "Processing 080120.txt\n",
      "Processing 090120.txt\n",
      "Processing 100120.txt\n",
      "Processing 110120.txt\n",
      "Processing 120120.txt\n",
      "Closing output files\n"
     ]
    }
   ],
   "source": [
    "file_handles = {}\n",
    "\n",
    "# Open one file per ticker, write\n",
    "print(\"Opening output files\")\n",
    "with open(\"./tickers_forexite.txt\") as tickers:\n",
    "    for ticker in tickers:\n",
    "        clean_ticker = ticker.strip()\n",
    "        file_handles[clean_ticker] = open(f\"./csv/{clean_ticker}.csv\", \"a\")\n",
    "\n",
    "print(\"Loading file list\")\n",
    "files = new_files\n",
    "if not files:\n",
    "    files = Path('./forexite/').glob('*.zip')\n",
    "files = sorted(list(files))\n",
    "\n",
    "print(\"Parsing files\")\n",
    "for file in files:\n",
    "    with zipfile.ZipFile(file) as z:\n",
    "        fn = z.namelist()[0]\n",
    "        print(f\"Processing {fn}\")\n",
    "        with z.open(fn) as f:\n",
    "            next(f)\n",
    "            for row in f:\n",
    "                clean_row = row.decode(\"utf-8\").strip()\n",
    "                ticker, ohlc = parse_row(clean_row)\n",
    "                if ticker not in file_handles:\n",
    "                    continue\n",
    "                file_handles[ticker].write(\"{}, {}, {}, {}, {}\\n\".format(*ohlc))\n",
    "                \n",
    "print(\"Closing output files\")\n",
    "for _, handle in file_handles.items():\n",
    "    handle.close()\n",
    "                \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
