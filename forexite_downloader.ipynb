{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading historical forex data from forexite\n",
    "The url I got from [here](https://www.quantshare.com/sa-421-6-places-to-download-historical-intraday-forex-quotes-data-for-free). It has 1m resolution data for [lots](./forexite_pairs.txt) of symbols. Eah file contains one days data for all symbols. I need to rework the data, one file per symbol. Store in pandas and serialize. Need to handle time zones somehow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import zipfile\n",
    "import urllib.request\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.forexite.com/free_forex_quotes/2020/02/220220.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/02/210220.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/02/200220.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/02/190220.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/02/180220.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/02/170220.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/02/160220.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/02/150220.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/02/140220.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/02/130220.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/02/120220.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/02/110220.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/02/100220.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/02/090220.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/02/080220.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/02/070220.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/02/060220.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/02/050220.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/02/040220.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/02/030220.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/02/020220.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/02/010220.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/01/310120.zip\n",
      "https://www.forexite.com/free_forex_quotes/2020/01/300120.zip\n"
     ]
    }
   ],
   "source": [
    "date = datetime.datetime.now()\n",
    "# e.g. url = \"https://www.forexite.com/free_forex_quotes/2011/11/011111.zip\"\n",
    "url = \"https://www.forexite.com/free_forex_quotes/{}/{:02d}/{:02d}{:02d}{:02d}.zip\"\n",
    "filename = \"./forexite/{:04d}-{:02d}-{:02d}.zip\"\n",
    "new_files = []\n",
    "\n",
    "for x in range(1, 365):\n",
    "    data_date = date - datetime.timedelta(days=x)\n",
    "    year = data_date.year\n",
    "    year_2 = int(str(year)[-2:])\n",
    "    month = data_date.month\n",
    "    day = data_date.day\n",
    "    download = url.format(year, month, day, month, year_2)\n",
    "    file = filename.format(year, month, day)\n",
    "    file_obj = Path(file)\n",
    "    if not file_obj.exists():\n",
    "        print(download)\n",
    "        if not Path('./forexite').is_dir():\n",
    "            Path('./forexite').mkdir()\n",
    "        urllib.request.urlretrieve(download, file)\n",
    "        new_files.append(file_obj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to csv files, one ticker per file\n",
    "I want one file for each currency pair. I would like it to read the current file, figure out if there is any new data and add it, and then save the file again. I will read and analyse the files in a different codepath.\n",
    "\n",
    "In fact, lets convert the forexite data into csv files first, these are human readable and pandas readable.\n",
    "Thoughts:\n",
    "- Do I dump everything into the csv, then sort and dedupe\n",
    "- Should I use a list of tuples, (datetime, o, h, l, c)\n",
    "- I could load the existing file (if it exists) into a set, each new file can check to see if it is duplicated\n",
    "- I only need to use the date to dedupe, so, from each file, get the date, check the set. If not there, we add all the rows to the list of tuples\n",
    "- Then we sort them and write them out\n",
    "- This might have quite high memory requirements, can we make some assumptions about the order the files are presented to us? Would be challenging if there were gaps in the data, is this an edge case?\n",
    "- Lets start with the first run case, we can optimise later if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes a filename\n",
    "# function that takes a row and a list\n",
    "def parse_row(row_from_file):\n",
    "    # Row looks like:\n",
    "    # EURUSD,20191101,000900,1.1150,1.1150,1.1150,1.1150\n",
    "    tokens = row_from_file.split(',')\n",
    "    assert len(tokens) == 7, f\"Not enough tokens in: {row_from_file}\"\n",
    "    \n",
    "    date_time_str = str(tokens[1]) + str(tokens[2])\n",
    "    dt = datetime.datetime.strptime(date_time_str, '%Y%m%d%H%M%S')\n",
    "    \n",
    "    ticker = tokens[0].strip()\n",
    "    opener = tokens[3].strip()\n",
    "    high = tokens[4].strip()\n",
    "    low = tokens[5].strip()\n",
    "    close = tokens[6].strip()\n",
    "    \n",
    "    return ticker, (dt, opener, high, low, close)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating file list\n",
      "Parsing available tickers and Opening output files\n",
      "Processing 220220.txt\n",
      "Parsing files\n",
      "Processing 300120.txt\n",
      "Processing 310120.txt\n",
      "Processing 010220.txt\n",
      "Processing 020220.txt\n",
      "Processing 030220.txt\n",
      "Processing 040220.txt\n",
      "Processing 050220.txt\n",
      "Processing 060220.txt\n",
      "Processing 070220.txt\n",
      "Processing 080220.txt\n",
      "Processing 090220.txt\n",
      "Processing 100220.txt\n",
      "Processing 110220.txt\n",
      "Processing 120220.txt\n",
      "Processing 130220.txt\n",
      "Processing 140220.txt\n",
      "Processing 150220.txt\n",
      "Processing 160220.txt\n",
      "Processing 170220.txt\n",
      "Processing 180220.txt\n",
      "Processing 190220.txt\n",
      "Processing 200220.txt\n",
      "Processing 210220.txt\n",
      "Processing 220220.txt\n",
      "Closing output files\n"
     ]
    }
   ],
   "source": [
    "file_handles = {}\n",
    "\n",
    "print(\"Generating file list\")\n",
    "#new_files = None # HACK!\n",
    "files = new_files\n",
    "if not new_files:\n",
    "    files = Path('./forexite/').glob('*.zip')\n",
    "files = sorted(list(files))\n",
    "\n",
    "\n",
    "if not Path('./csv').is_dir():\n",
    "    print(\"Creating output directory\")\n",
    "    Path('./csv').mkdir()\n",
    "    \n",
    "print(\"Parsing available tickers and Opening output files\")\n",
    "with zipfile.ZipFile(files[-1]) as z:\n",
    "    fn = z.namelist()[0]\n",
    "    print(f\"Processing {fn}\")\n",
    "    with z.open(fn) as f:\n",
    "        next(f)\n",
    "        for row in f:\n",
    "            clean_row = row.decode(\"utf-8\").strip()\n",
    "            ticker, ohlc = parse_row(clean_row)\n",
    "            if ticker not in file_handles:\n",
    "                file_handles[ticker] = open(f\"./csv/{ticker}.csv\", \"a\")\n",
    "\n",
    "print(\"Parsing files\")\n",
    "for file in files:\n",
    "    with zipfile.ZipFile(file) as z:\n",
    "        fn = z.namelist()[0]\n",
    "        print(f\"Processing {fn}\")\n",
    "        with z.open(fn) as f:\n",
    "            next(f)\n",
    "            for row in f:\n",
    "                clean_row = row.decode(\"utf-8\").strip()\n",
    "                ticker, ohlc = parse_row(clean_row)\n",
    "                if ticker not in file_handles:\n",
    "                    continue\n",
    "                file_handles[ticker].write(\"{}, {}, {}, {}, {}\\n\".format(*ohlc))\n",
    "                \n",
    "print(\"Closing output files\")\n",
    "for _, handle in file_handles.items():\n",
    "    handle.close()\n",
    "                \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
